# Supercheck - Local Development (Build from Source)
#
# Use this for active development. Builds images from local source code
# with hot-reload support for faster iteration.
#
# Quick Start:
#   1. Clone the repository
#   2. Run: docker compose -f docker-compose-local.yml up -d --build
#   3. Access: http://localhost:3000
#
# Features:
#   - Builds app and worker from local source (../../app, ../../worker)
#   - Source code mounted for hot-reload
#   - All services included: PostgreSQL, Redis, MinIO
#
# For production deployment, use docker-compose-secure.yml instead.
#
# Documentation: https://supercheck.io/docs/deployment/self-hosted
# ============================================================================

# Project name ensures consistent volume naming across different directories
name: supercheck

# YAML anchors for shared configuration
x-common-env: &common-env # Database Configuration
  DATABASE_URL: ${DATABASE_URL:-postgresql://postgres:postgres@postgres:5432/supercheck}
  DB_HOST: ${DB_HOST:-postgres}
  DB_PORT: ${DB_PORT:-5432}
  DB_USER: ${DB_USER:-postgres}
  DB_PASSWORD: ${DB_PASSWORD:-postgres}
  DB_NAME: ${DB_NAME:-supercheck}

  # Redis Configuration
  REDIS_HOST: ${REDIS_HOST:-redis}
  REDIS_PORT: ${REDIS_PORT:-6379}
  REDIS_PASSWORD: ${REDIS_PASSWORD:-supersecure-redis-password-change-this}
  REDIS_URL: ${REDIS_URL:-redis://:supersecure-redis-password-change-this@redis:6379}
  REDIS_TLS_ENABLED: ${REDIS_TLS_ENABLED:-false}
  REDIS_TLS_REJECT_UNAUTHORIZED: ${REDIS_TLS_REJECT_UNAUTHORIZED:-false}

  # App Configuration
  NEXT_PUBLIC_APP_URL: ${NEXT_PUBLIC_APP_URL:-http://localhost:3000}
  BETTER_AUTH_URL: ${BETTER_AUTH_URL:-http://localhost:3000}

  # Status Page Domain Configuration
  STATUS_PAGE_DOMAIN: ${STATUS_PAGE_DOMAIN:-localhost}
  BETTER_AUTH_SECRET: ${BETTER_AUTH_SECRET:-CHANGE_THIS_GENERATE_32_CHAR_HEX}
  NODE_ENV: ${NODE_ENV:-production}
  # Parallel Execution Capacity (self-hosted mode)
  # Cloud mode uses plan limits: Plus=5/50, Pro=10/100
  # For self-hosted, match RUNNING_CAPACITY to WORKER_REPLICAS (each worker handles 1 job)
  # Example: WORKER_REPLICAS=2 RUNNING_CAPACITY=2 docker-compose up -d
  RUNNING_CAPACITY: ${RUNNING_CAPACITY:-1} # Max concurrent executions
  QUEUED_CAPACITY: ${QUEUED_CAPACITY:-10} # Max jobs waiting in queue
  # Container limits for 2 vCPU / 4GB servers
  CONTAINER_CPU_LIMIT: ${CONTAINER_CPU_LIMIT:-1.5}
  CONTAINER_MEMORY_LIMIT_MB: ${CONTAINER_MEMORY_LIMIT_MB:-2048}
  TEST_EXECUTION_TIMEOUT_MS: ${TEST_EXECUTION_TIMEOUT_MS:-300000}
  JOB_EXECUTION_TIMEOUT_MS: ${JOB_EXECUTION_TIMEOUT_MS:-3600000}
  K6_TEST_EXECUTION_TIMEOUT_MS: ${K6_TEST_EXECUTION_TIMEOUT_MS:-3600000}
  K6_JOB_EXECUTION_TIMEOUT_MS: ${K6_JOB_EXECUTION_TIMEOUT_MS:-3600000}

  # For local dev - processes all queues
  WORKER_LOCATION: ${WORKER_LOCATION:-local}

  # AWS S3 / MinIO Configuration
  AWS_REGION: ${AWS_REGION:-us-east-1}
  AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID:-minioadmin}
  AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY:-minioadmin}
  S3_ENDPOINT: ${S3_ENDPOINT:-http://minio:9000}
  # Playwright buckets
  S3_TEST_BUCKET_NAME: ${S3_TEST_BUCKET_NAME:-playwright-test-artifacts}
  S3_JOB_BUCKET_NAME: ${S3_JOB_BUCKET_NAME:-playwright-job-artifacts}
  S3_MONITOR_BUCKET_NAME: ${S3_MONITOR_BUCKET_NAME:-playwright-monitor-artifacts}
  # K6 buckets
  S3_K6_TEST_BUCKET_NAME: ${S3_K6_TEST_BUCKET_NAME:-k6-test-artifacts}
  S3_K6_JOB_BUCKET_NAME: ${S3_K6_JOB_BUCKET_NAME:-k6-job-artifacts}
  # Status page bucket
  S3_STATUS_BUCKET_NAME: ${S3_STATUS_BUCKET_NAME:-status-page-artifacts}
  S3_FORCE_PATH_STYLE: ${S3_FORCE_PATH_STYLE:-true}
  S3_OPERATION_TIMEOUT: ${S3_OPERATION_TIMEOUT:-10000}
  S3_MAX_RETRIES: ${S3_MAX_RETRIES:-3}

  # Notification Channel Limits
  # Notification channel limits (runtime configurable)
  MAX_JOB_NOTIFICATION_CHANNELS: ${MAX_JOB_NOTIFICATION_CHANNELS:-10}
  MAX_MONITOR_NOTIFICATION_CHANNELS: ${MAX_MONITOR_NOTIFICATION_CHANNELS:-10}

  # Playwright Configuration - Optimized for Supercheck execution service
  # PLAYWRIGHT_HEADLESS: ${PLAYWRIGHT_HEADLESS:-true}
  PLAYWRIGHT_WORKERS: ${PLAYWRIGHT_WORKERS:-1} # Single worker for 2GB container memory
  PLAYWRIGHT_RETRIES: ${PLAYWRIGHT_RETRIES:-1}
  PLAYWRIGHT_TRACE: ${PLAYWRIGHT_TRACE:-retain-on-failure}
  PLAYWRIGHT_SCREENSHOT: ${PLAYWRIGHT_SCREENSHOT:-on}
  # Video recording enabled on test failures for debugging (with increased resource limits)
  PLAYWRIGHT_VIDEO: ${PLAYWRIGHT_VIDEO:-retain-on-failure}

  # Monitor Results Cleanup (retention per plan from database)
  MONITOR_CLEANUP_ENABLED: ${MONITOR_CLEANUP_ENABLED:-true}
  MONITOR_CLEANUP_CRON: ${MONITOR_CLEANUP_CRON:-"0 2 * * *"} # 2 AM daily
  MONITOR_CLEANUP_BATCH_SIZE: ${MONITOR_CLEANUP_BATCH_SIZE:-1000}
  MONITOR_PRESERVE_STATUS_CHANGES: ${MONITOR_PRESERVE_STATUS_CHANGES:-true}
  MONITOR_CLEANUP_SAFETY_LIMIT: ${MONITOR_CLEANUP_SAFETY_LIMIT:-1000000}

  # Monitor Aggregates Cleanup (retention per plan from database)
  MONITOR_AGGREGATES_CLEANUP_ENABLED: ${MONITOR_AGGREGATES_CLEANUP_ENABLED:-true}
  MONITOR_AGGREGATES_CLEANUP_CRON: ${MONITOR_AGGREGATES_CLEANUP_CRON:-"30 2 * * *"} # 2:30 AM daily
  MONITOR_AGGREGATES_CLEANUP_BATCH_SIZE: ${MONITOR_AGGREGATES_CLEANUP_BATCH_SIZE:-1000}
  MONITOR_AGGREGATES_CLEANUP_SAFETY_LIMIT: ${MONITOR_AGGREGATES_CLEANUP_SAFETY_LIMIT:-500000}

  # Job Runs Cleanup (retention per plan from database)
  JOB_RUNS_CLEANUP_ENABLED: ${JOB_RUNS_CLEANUP_ENABLED:-true}
  JOB_RUNS_CLEANUP_CRON: ${JOB_RUNS_CLEANUP_CRON:-"0 3 * * *"} # 3 AM daily
  JOB_RUNS_CLEANUP_BATCH_SIZE: ${JOB_RUNS_CLEANUP_BATCH_SIZE:-100}
  JOB_RUNS_CLEANUP_SAFETY_LIMIT: ${JOB_RUNS_CLEANUP_SAFETY_LIMIT:-10000}

  # Playground Artifacts Cleanup
  PLAYGROUND_CLEANUP_ENABLED: ${PLAYGROUND_CLEANUP_ENABLED:-true}
  PLAYGROUND_CLEANUP_CRON: ${PLAYGROUND_CLEANUP_CRON:-"0 5 * * *"} # 5 AM daily
  PLAYGROUND_CLEANUP_MAX_AGE_HOURS: ${PLAYGROUND_CLEANUP_MAX_AGE_HOURS:-24}

  # Webhook Idempotency Cleanup (TTL-based deduplication records)
  WEBHOOK_CLEANUP_ENABLED: ${WEBHOOK_CLEANUP_ENABLED:-true}
  WEBHOOK_CLEANUP_CRON: ${WEBHOOK_CLEANUP_CRON:-"0 4 * * *"} # 4 AM daily
  WEBHOOK_CLEANUP_BATCH_SIZE: ${WEBHOOK_CLEANUP_BATCH_SIZE:-1000}
  WEBHOOK_CLEANUP_SAFETY_LIMIT: ${WEBHOOK_CLEANUP_SAFETY_LIMIT:-100000}

  # Security Configuration
  SECRET_ENCRYPTION_KEY: ${SECRET_ENCRYPTION_KEY:-CHANGE_THIS_GENERATE_32_CHAR_HEX}

  # Better Auth Admin Configuration
  MAX_PROJECTS_PER_ORG: ${MAX_PROJECTS_PER_ORG:-10}
  DEFAULT_PROJECT_NAME: ${DEFAULT_PROJECT_NAME:-Default Project}

  # SMTP Email Configuration for Notifications
  # Supports any SMTP provider including Resend SMTP, Gmail, SendGrid, etc.
  SMTP_HOST: ${SMTP_HOST:-smtp.resend.com}
  SMTP_PORT: ${SMTP_PORT:-587}
  SMTP_USER: ${SMTP_USER:-resend}
  SMTP_PASSWORD: ${SMTP_PASSWORD:-your-smtp-password-change-this-in-production}
  SMTP_SECURE: ${SMTP_SECURE:-false}
  SMTP_FROM_EMAIL: ${SMTP_FROM_EMAIL:-notification@example.com}

  # ==========================================================================
  # AI Features Configuration (AI Fix, AI Create, AI Analyze)
  # ==========================================================================
  # --------------------------------------------------------------------------
  # AI Model Configuration
  # Set AI_PROVIDER to one of: openai, azure, anthropic, gemini, google-vertex, bedrock, openrouter
  # --------------------------------------------------------------------------
  AI_PROVIDER: ${AI_PROVIDER:-openai}
  AI_MODEL: ${AI_MODEL:-gpt-4o-mini}
  AI_TIMEOUT_MS: ${AI_TIMEOUT_MS:-90000}
  AI_MAX_RETRIES: ${AI_MAX_RETRIES:-2}
  AI_TEMPERATURE: ${AI_TEMPERATURE:-0.1}

  # --------------------------------------------------------------------------
  # Provider A: OpenAI (default)
  # Get your API key from: https://platform.openai.com/api-keys
  # --------------------------------------------------------------------------
  OPENAI_API_KEY: ${OPENAI_API_KEY:-}

  # --------------------------------------------------------------------------
  # Provider B: Azure OpenAI
  # Setup: https://portal.azure.com → Create Azure OpenAI resource
  # Get keys from: Azure Portal → Your Resource → Keys and Endpoint
  # --------------------------------------------------------------------------
  AZURE_RESOURCE_NAME: ${AZURE_RESOURCE_NAME:-}
  AZURE_API_KEY: ${AZURE_API_KEY:-}
  AZURE_OPENAI_DEPLOYMENT: ${AZURE_OPENAI_DEPLOYMENT:-}
  AZURE_API_VERSION: ${AZURE_API_VERSION:-2024-06-01}
  AZURE_USE_MANAGED_IDENTITY: ${AZURE_USE_MANAGED_IDENTITY:-false}

  # --------------------------------------------------------------------------
  # Provider C: Anthropic Claude
  # Get your API key from: https://console.anthropic.com/settings/keys
  # --------------------------------------------------------------------------
  ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY:-}

  # --------------------------------------------------------------------------
  # Provider D: Google Gemini (AI Studio - simple API key, recommended)
  # Get your API key from: https://aistudio.google.com/apikey
  # --------------------------------------------------------------------------
  GOOGLE_GENERATIVE_AI_API_KEY: ${GOOGLE_GENERATIVE_AI_API_KEY:-}

  # --------------------------------------------------------------------------
  # Provider E: Google Vertex AI (GCP enterprise - requires GCP project)
  # Setup: https://console.cloud.google.com/vertex-ai
  # Requires: GCP project + ADC authentication
  # --------------------------------------------------------------------------
  GOOGLE_VERTEX_PROJECT: ${GOOGLE_VERTEX_PROJECT:-}
  GOOGLE_VERTEX_LOCATION: ${GOOGLE_VERTEX_LOCATION:-us-central1}

  # --------------------------------------------------------------------------
  # Provider F: AWS Bedrock
  # Setup: https://console.aws.amazon.com/bedrock
  # Uses BEDROCK_* prefix to avoid conflicts with S3 credentials
  # On AWS infrastructure: leave credentials empty to use IAM role
  # --------------------------------------------------------------------------
  BEDROCK_AWS_REGION: ${BEDROCK_AWS_REGION:-}
  BEDROCK_AWS_ACCESS_KEY_ID: ${BEDROCK_AWS_ACCESS_KEY_ID:-}
  BEDROCK_AWS_SECRET_ACCESS_KEY: ${BEDROCK_AWS_SECRET_ACCESS_KEY:-}
  BEDROCK_AWS_SESSION_TOKEN: ${BEDROCK_AWS_SESSION_TOKEN:-}

  # --------------------------------------------------------------------------
  # Provider G: OpenRouter (unified gateway to 400+ models)
  # Get your API key from: https://openrouter.ai/keys
  # Model IDs use provider/model format, e.g. anthropic/claude-3.5-haiku
  # --------------------------------------------------------------------------
  OPENROUTER_API_KEY: ${OPENROUTER_API_KEY:-}

  # Self-Hosted Mode (set to 'true' for unlimited features)
  # Client-side code fetches this from /api/config/hosting-mode at runtime
  SELF_HOSTED: ${SELF_HOSTED:-true}

  # OAuth Configuration (optional - enable social login)
  # Social auth buttons are automatically shown when client ID and secret are configured
  GITHUB_CLIENT_ID: ${GITHUB_CLIENT_ID:-}
  GITHUB_CLIENT_SECRET: ${GITHUB_CLIENT_SECRET:-}
  GOOGLE_CLIENT_ID: ${GOOGLE_CLIENT_ID:-}
  GOOGLE_CLIENT_SECRET: ${GOOGLE_CLIENT_SECRET:-}

  # CAPTCHA Configuration (optional - Cloudflare Turnstile)
  # CAPTCHA is automatically enabled when both keys are set
  # Get keys from https://dash.cloudflare.com/?to=/:account/turnstile
  TURNSTILE_SITE_KEY: ${TURNSTILE_SITE_KEY:-}
  TURNSTILE_SECRET_KEY: ${TURNSTILE_SECRET_KEY:-}

  # Queue Alerting Configuration (optional)
  QUEUE_ALERTING_ENABLED: ${QUEUE_ALERTING_ENABLED:-true}
  # Alerts: High depth (>70%), Long wait (>15m), High failure (>5%), Stall detection
  QUEUE_ALERT_SLACK_WEBHOOK_URL: ${QUEUE_ALERT_SLACK_WEBHOOK_URL:-}
  QUEUE_ALERT_WEBHOOK_URL: ${QUEUE_ALERT_WEBHOOK_URL:-} # JSON payload { type: 'queue_alert', alert: ... }

services:
  # App (Next.js Frontend)
  app:
    build:
      context: ../../app
      dockerfile: Dockerfile
    image: supercheck-app:latest
    ports:
      - "3000:3000"
    environment:
      <<: *common-env
    volumes:
      # Mount TypeScript definitions for Monaco Editor in playground
      - ../../app/public/supercheck.d.ts:/app/public/supercheck.d.ts:ro
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      minio:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://127.0.0.1:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    networks:
      - supercheck-network
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 2G
        reservations:
          cpus: "0.5"
          memory: 1G

  # Worker (NestJS Runner) - Container-based test execution
  # Tests run in isolated Docker containers (Playwright/K6), not inside worker
  # Playwright Docker best practices: https://playwright.dev/docs/docker
  worker:
    build:
      context: ../../worker
      dockerfile: Dockerfile
    image: supercheck-worker:latest
    # init: Use tini as PID 1 to avoid zombie processes (Playwright recommendation)
    init: true
    environment:
      <<: *common-env
      WORKER_IMAGE: supercheck-worker:latest
      # Note: MAX_CONCURRENT_EXECUTIONS=1 is hardcoded in worker code (scale via replicas)
    volumes:
      # Mount Docker socket for container-based execution
      # Worker spawns isolated containers for Playwright and K6 tests
      - /var/run/docker.sock:/var/run/docker.sock:ro # Read-only for security
      - worker-playwright-reports:/worker/playwright-reports
      - worker-k6-reports:/worker/k6-reports
      - worker-reports:/worker/report
      # Development: Mount source for hot reload (remove in production)
      - ../../worker/src:/worker/src:ro
    depends_on:
      postgres:
        condition: service_healthy
      app:
        condition: service_healthy
      redis:
        condition: service_healthy
      minio:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://127.0.0.1:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - supercheck-network
    deploy:
      replicas: ${WORKER_REPLICAS:-1} # Single worker for local development
      resources:
        # Resource limits for 2 vCPU / 4GB servers
        # Worker spawns containers with 1.5 CPU + 2GB RAM
        limits:
          cpus: "1.8"
          memory: 3G
        reservations:
          cpus: "0.5"
          memory: 1G
    # Security constraints - maintain production security locally
    security_opt:
      - no-new-privileges:true # Prevent privilege escalation
    cap_drop:
      - ALL # Drop all capabilities by default
    # Note: SYS_ADMIN capability removed - not needed for container execution
    # Browsers run in separate containers with their own capabilities

  # PostgreSQL Database
  postgres:
    image: postgres:18
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=supercheck
    volumes:
      - postgres-data:/var/lib/postgresql/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 15
      start_period: 30s
    networks:
      - supercheck-network
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 1.5G
        reservations:
          cpus: "0.5"
          memory: 1G

  # Redis (for job queues)
  redis:
    image: redis:8
    environment:
      - REDIS_PASSWORD=supersecure-redis-password-change-this
    command: sh -c "rm -rf /data/* && redis-server --requirepass \"supersecure-redis-password-change-this\" --maxmemory 512mb --maxmemory-policy noeviction --save '' --appendonly no --protected-mode yes --bind 0.0.0.0 --tcp-keepalive 300"
    volumes:
      - redis-data:/data
    restart: unless-stopped
    healthcheck:
      test:
        [
          "CMD",
          "redis-cli",
          "-a",
          "supersecure-redis-password-change-this",
          "ping",
        ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - supercheck-network
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 512M
        reservations:
          cpus: "0.25"
          memory: 256M

  # MinIO (S3-compatible storage)
  minio:
    image: minio/minio:latest
    environment:
      - MINIO_ROOT_USER=minioadmin
      - MINIO_ROOT_PASSWORD=minioadmin
    command: server /data --console-address ":9001"
    volumes:
      - minio-data:/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "mc", "ready", "local"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - supercheck-network
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 1G
        reservations:
          cpus: "0.25"
          memory: 512M

volumes:
  postgres-data:
    driver: local
  redis-data:
    driver: local
  minio-data:
    driver: local
  worker-playwright-reports:
    driver: local
  worker-k6-reports:
    driver: local
  worker-reports:
    driver: local

networks:
  supercheck-network:
    driver: bridge
