---
title: Dashboard
description: Project overview, real-time metrics, and performance analytics
icon: ChartColumn
---

The Dashboard provides a comprehensive view of your project's health with real-time insights into tests, jobs, monitors, and detailed analytics for both Playwright and K6 performance tests.

![Dashboard](/screenshots/dashboard.png)

## Overview Tab

The Overview tab shows key project metrics at a glance.

### Key Metrics

| Metric | Description |
|--------|-------------|
| **Total Tests** | Test cases saved in your project |
| **Active Jobs** | Scheduled or running jobs |
| **Active Monitors** | Enabled monitoring endpoints |
| **Total Runs** | Job executions (last 30 days) |
| **Execution Time** | Total test time (last 30 days) |

### Charts

#### Job Runs
Bar chart showing successful vs failed job runs over the last 30 days. Quickly identify failure patterns and trends.

#### Monitor Status
Current health distribution of all monitors:
- **Up** — Monitors passing health checks
- **Down** — Monitors failing health checks

#### Test Types
Pie chart breakdown of test types in your project:
- Browser, API, Database, Custom, Performance

#### Job Activity
Stacked area chart showing job triggers by type:
- **Manual** — User-initiated runs
- **Scheduled** — Cron-based runs
- **Remote** — API-triggered runs (CI/CD)

#### Uptime Trend
Line chart showing monitor uptime percentage over 30 days. Track reliability trends over time.

---

## K6 Analytics Tab

The K6 Analytics tab provides detailed performance metrics for your load tests.

![K6 Analytics](/screenshots/dashboard-k6-analytics.png)

### Filters

- **Job** — Filter to a specific K6 job or view all jobs
- **Period** — Analyze last 30, 60, or 90 days of data

### Metric Cards

| Metric | Description |
|--------|-------------|
| **Total Runs** | Number of K6 test executions in the selected period |
| **Passed** | Runs where all thresholds passed, with pass rate percentage |
| **Failed** | Runs with threshold failures (highlighted if attention needed) |
| **Avg P95 Response** | Average 95th percentile response time across runs |
| **Avg Request Rate** | Average requests per second across runs |
| **Avg Response Time** | Mean response latency across runs |

### Charts

#### Response Time Trend
Area chart showing P95 response time across runs. Use this to identify performance regressions or improvements over time.

#### Request Rate Trend
Area chart showing requests per second across runs. Monitor throughput patterns and capacity trends.

---

### Run Comparison

When a specific job is selected and has 2+ runs, click **Compare Runs** to open a side-by-side comparison dialog.

![Run Comparison Dialog](/screenshots/dashboard-k6-compare-dialog.png)

#### Run Selection

- **Baseline (Left)** — Select the reference run to compare against
- **Compare To (Right)** — Select the run to compare with the baseline

Each run in the dropdown displays:
- Pass/fail status indicator (green checkmark or red X)
- Run date and time
- P95 response time
- Run ID

#### Comparison Metrics

The comparison table shows metrics from both runs with calculated delta values:

| Metric | Description | Better When |
|--------|-------------|-------------|
| **P95 Response Time** | 95th percentile latency | Lower |
| **P99 Response Time** | 99th percentile latency | Lower |
| **Avg Response Time** | Mean latency | Lower |
| **Total Requests** | Total HTTP requests made | Higher (more throughput) |
| **Failed Requests** | Number of failed requests | Lower |
| **Request Rate** | Requests per second | Higher (more throughput) |
| **Peak VUs** | Maximum virtual users reached | Context-dependent |
| **Duration** | Test execution time | Context-dependent |

Delta values are color-coded:
- **Green** — Improvement (lower latency or higher throughput)
- **Red** — Regression (higher latency or lower throughput)
- Percentage changes displayed for additional context

---

### View k6 Reports

Click **View k6 Reports** to compare the detailed k6 HTML reports side-by-side. This button is always visible when two runs are selected.

![K6 Report Comparison](/screenshots/dashboard-k6-report-comparision.png)

The side-by-side view displays:
- Full k6 HTML report for the baseline run (left)
- Full k6 HTML report for the comparison run (right)
- If a report is not available for a run, a placeholder message is shown instead
- Click **Back to Metrics** to return to the comparison table

Use this view to drill into detailed request timings, thresholds, checks, and HTTP response details for each run.

---

### AI Analyze

Click **AI Analyze** to generate an AI-powered performance comparison report. This feature analyzes both runs and provides insights on performance differences, potential causes, and recommendations.

![AI Performance Analysis](/screenshots/dashboard-k6-ai-report-comparision.png)

#### How It Works

1. Click the **AI Analyze** button (purple sparkles icon)
2. The analysis streams in real-time as it's generated
3. View the detailed markdown report in the analysis viewer

#### Analysis Report Contents

The AI generates a professional performance comparison report that includes:
- **Executive Summary** — High-level findings and conclusions
- **Metric Comparison** — Detailed analysis of response times, throughput, and error rates
- **Performance Insights** — What improved, what regressed, and by how much
- **Potential Causes** — AI-identified reasons for performance changes
- **Recommendations** — Actionable steps to improve performance

#### Report Actions

- **Copy** — Copy the full report to clipboard
- **Download** — Download as a markdown file (`.md`)
- **Close** — Return to the comparison dialog

<Callout type="info">
AI Analyze requires an OpenAI API key configured in your environment. See [AI Features](/docs/deployment/self-hosted#ai-features) for setup instructions.
</Callout>

---

## Playwright Analytics Tab

The Playwright Analytics tab provides insights into your browser test execution patterns and duration trends.

![Playwright Analytics](/screenshots/dashboard-playwright-analytics.png)

### Filters

- **Job** — Filter to a specific Playwright job or view all jobs
- **Period** — Analyze last 30, 60, or 90 days of data

### Metric Cards

| Metric | Description |
|--------|-------------|
| **Total Runs** | Number of Playwright test executions in the selected period |
| **Passed** | Successful runs with pass rate percentage |
| **Failed** | Failed runs (highlighted if attention needed) |
| **Avg Duration** | Average test execution time per run |
| **P95 Duration** | 95th percentile execution time (slowest 5% of runs) |
| **Total Duration** | Cumulative execution time in minutes |

### Charts

#### Run Frequency
Stacked bar chart showing daily test run distribution:
- **Green bars** — Passed runs
- **Red bars** — Failed runs

Use this to identify testing patterns, coverage gaps, and failure spikes.

#### Execution Time Trend
Area chart showing test duration over time. Monitor for:
- Performance regressions in your test suite
- Flaky tests causing variable execution times
- Infrastructure issues affecting test speed

---

## Best Practices

- **Review daily** — Check key metrics for anomalies
- **Investigate failures** — Use Job Runs chart to spot patterns
- **Track uptime** — Monitor Uptime Trend for SLA compliance
- **Balance triggers** — Use Job Activity to ensure scheduled runs are working
- **Compare before deploying** — Use K6 Run Comparison to validate performance before releases
- **Use AI Analyze** — Get quick insights when comparing K6 performance runs
- **Monitor P95 trends** — Catch performance regressions early
- **Download reports** — Save AI analysis reports for documentation and post-mortems
