---
title: Quick Start
description: Get started with Supercheck in minutes
---

This guide walks you through creating your first test, setting up monitoring, and configuring alerts. By the end, you'll have automated testing and monitoring running for your application.

## Prerequisites

- A running Supercheck instance ([self-hosted](./deployment) or cloud)
- A web application or API to test

## Step 1: Create Your First Test

The fastest way to create a test is using the Playground with AI assistance.

### Option A: Use AI Create

1. Go to **Create â†’ Browser Test** to open the Playground
2. Click the **AI Create** button (purple gradient)
3. Describe what you want to test:
   ```
   Test that the homepage loads and displays the main heading
   ```
4. Review the generated code
5. Click **Accept** to use it

![AI Create Prompt](/screenshots/ai-create-prompt.png)

<Callout type="info">
  **Tip**: AI Create for browser tests is in beta. For complex flows, consider
  recording your test first with [Playwright
  Codegen](https://playwright.dev/docs/codegen) or [Playwright Browser
  Extension](https://chromewebstore.google.com/detail/playwright-crx/jambeljnbnfbkcpnoiaedcabbgmnnlcd?pli=1),
  then use AI to enhance it.
</Callout>

### Option B: Use a Template

1. Go to **Create â†’ Browser Test**
2. Click **Templates** in the toolbar
3. Select a template (e.g., "Basic Page Load")
4. Customize the URL and assertions

![Code Template](/screenshots/code-template.png)

### Option C: Write from Scratch

Copy this example and modify for your application:

<Tabs items={['Browser Test', 'API Test', 'Performance Test']}>
<Tab value="Browser Test">
```typescript
/**
 * Playwright UI smoke test.
 * 
 * Purpose:
 * - Verify that the application loads correctly
 * - Check that critical UI elements are visible
 * - Perform a basic user interaction
 * 
 * @see https://playwright.dev/docs/writing-tests
 */
import { expect, test } from '@playwright/test';

const APP_URL = 'https://demo.playwright.dev/todomvc';

test.describe('UI smoke test', () => {
test('home page renders primary UI', async ({ page }) => {
// Navigate to the application
await page.goto(APP_URL);

    // Verify page title and input visibility
    await expect(page).toHaveTitle(/TodoMVC/);
    await expect(page.getByPlaceholder('What needs to be done?')).toBeVisible();

    // Perform interaction: Add a new task
    await page.getByPlaceholder('What needs to be done?').fill('Smoke task');
    await page.keyboard.press('Enter');

    // Verify the task was added to the list
    await expect(page.getByRole('listitem').first()).toContainText('Smoke task');

});
});

````

**What this tests:**
- Page loads without errors
- Title contains expected text
- Input field is visible and functional
- User interaction works correctly

ðŸ“š [Playwright Writing Tests Guide](https://playwright.dev/docs/writing-tests)
</Tab>
<Tab value="API Test">
```typescript
/**
 * API health probe with contract checks.
 *
 * Purpose:
 * - Verify that the API is up and running (status 200)
 * - Check that the response headers are correct (Content-Type)
 * - Validate the structure and data types of the response body
 *
 * @see https://playwright.dev/docs/api-testing
 */
import { expect, test } from '@playwright/test';

const API_URL = 'https://jsonplaceholder.typicode.com';

test.describe('API health check', () => {
  test('health endpoint responds with expected payload', async ({ request }) => {
    // Send GET request
    const response = await request.get(API_URL + '/posts/1');

    // Basic status checks
    expect(response.ok()).toBeTruthy();
    expect(response.status()).toBe(200);
    expect(response.headers()['content-type']).toContain('application/json');

    // Validate JSON structure
    const body = await response.json();
    expect(body).toMatchObject({ id: 1 });
    expect(typeof body.title).toBe('string');
  });
});
````

**What this tests:**

- API endpoint is reachable
- Returns HTTP 200 with correct content type
- Response body has expected structure

ðŸ“š [Playwright API Testing Guide](https://playwright.dev/docs/api-testing)

</Tab>
<Tab value="Performance Test">
```javascript
/**
 * k6 smoke test for uptime and latency.
 * 
 * Purpose:
 * - Verify system availability (uptime)
 * - Check basic latency performance
 * - Ensure the system is ready for heavier load tests
 * 
 * Configuration:
 * - VUs: 3 virtual users running concurrently
 * - Duration: 30 seconds test run
 * - Thresholds: Error rate < 1%, 95th percentile < 800ms
 * 
 * @see https://grafana.com/docs/k6/latest/using-k6/thresholds/
 */
import http from 'k6/http';
import { check, sleep } from 'k6';

export const options = {
  vus: 3,           // 3 concurrent users
  duration: '30s',  // Run for 30 seconds
  thresholds: {
    http_req_failed: ['rate<0.01'],      // Error rate < 1%
    http_req_duration: ['p(95)<800'],    // 95% of requests < 800ms
  },
};

export default function () {
  const baseUrl = 'https://test-api.k6.io';
  const response = http.get(baseUrl + '/public/crocodiles/1/');

// Validate response
check(response, {
'status is 200': (res) => res.status === 200,
'body is not empty': (res) => res.body && res.body.length > 0,
});

sleep(1); // Pause between requests
}

````

**What this tests:**
- API handles concurrent load
- Response times stay under threshold
- Error rate stays below 1%

ðŸ“š [k6 Getting Started](https://grafana.com/docs/k6/latest/) | [k6 Thresholds](https://grafana.com/docs/k6/latest/using-k6/thresholds/)
</Tab>
</Tabs>

## Step 2: Run and Debug

1. Click **Run** to execute your test
2. Watch the execution in real-time
3. View the report when complete:
   - **Screenshots** â€” Visual state at each step
   - **Trace** â€” Step-by-step replay
   - **Logs** â€” Console output and errors

![Playground Editor](/screenshots/playground-editor.png)

![Playground Report](/screenshots/playground-report.png)

### If Your Test Fails

1. Check the **screenshot** to see what the page looked like
2. Use the **trace viewer** to step through each action
3. Click **AI Fix** to get automatic suggestions for common issues:
   - Wrong selectors
   - Timing problems
   - Assertion mismatches

## Step 3: Save Your Test

<Callout type="warning">
**Only passing tests can be saved.** The Save button is disabled until your test executes successfully. If your test fails, fix the issues and run it again.
</Callout>

Once your test passes:

1. Click **Save** in the Playground
2. Enter a descriptive name (e.g., "Homepage Load Test")
3. Select the project to save to
4. Click **Save**

Your test is now stored and can be added to jobs or monitors.

## Step 4: Create a Scheduled Job

Jobs run your tests automatically on a schedule.

1. Go to **Create â†’ Job**
2. Enter a name (e.g., "Nightly Regression")
3. Select the tests to include

![Create Job Form](/screenshots/create-job-form.png)
![Select Tests for Job](/screenshots/create-job-select-tests.png)

4. Configure the schedule from the dropdown.
5. Click **Save**

## Step 5: Set Up Alerts

Get notified when tests fail.

### Add a Notification Provider

1. Go to **Alerts â†’ Notification Channels**
2. Click **Add Provider**

![Notification Channels](/screenshots/notification-channels.png)

3. Choose your preferred channel:

| Provider | Setup |
|----------|-------|
| **Slack** | Paste your webhook URL |
| **Email** | Configure SMTP settings |
| **Discord** | Paste your webhook URL |
| **Telegram** | Enter bot token and chat ID |
| **Webhook** | Enter your endpoint URL |

4. Click **Test** to verify
5. Click **Save**

### Attach Alerts to Your Job

1. Edit your job
2. Go to the **Alerts** section
3. Select your notification provider
4. Enable **Alert on Failure**

![Create Job Alerts Config](/screenshots/create-job-alerts-config.png)

5. Save

## Step 6: Create a Monitor (Optional)

Monitors check your services continuously, independent of your test suite.

1. Go to **Create â†’ Monitor**
2. Choose monitor type:
   - **HTTP** â€” For APIs and endpoints
   - **Website** â€” For web pages with SSL tracking
   - **Ping** â€” For server availability
   - **Port** â€” For service connectivity
   - **Synthetic** â€” For full Playwright tests

![Create HTTP Monitor](/screenshots/create-http-monitor.png)

3. Enter the target URL
4. Set check interval (e.g., 5 minutes)
5. Configure alert thresholds
6. Select notification providers
7. Click **Save**

The monitor starts checking immediately.

## What You've Built

After completing this guide, you have:

- âœ… **A test** that validates your application
- âœ… **A scheduled job** that runs tests automatically
- âœ… **Alerts** that notify you of failures
- âœ… **A monitor** (optional) for continuous uptime tracking

## Next Steps

<Cards>
<Card icon={<FlaskConical className="text-purple-500" />} title="Playground" href="/docs/app/automate/playground">
Learn more about AI Create, AI Fix, and the test editor
</Card>

<Card icon={<Globe className="text-sky-500" />} title="Monitors" href="/docs/app/monitor">
Set up comprehensive uptime monitoring
</Card>

<Card icon={<Variable className="text-rose-500" />} title="Variables" href="/docs/app/automate/variables">
Store API keys and secrets securely
</Card>

<Card icon={<Tally4 className="text-emerald-500" />} title="Status Pages" href="/docs/app/communicate/status-pages">
Create public status pages for your users
</Card>
</Cards>

## Common Questions

### How do I test with authentication?

Use [Variables](/docs/app/automate/variables) to store credentials:

```typescript
/**
 * Login test using secure credentials from project variables.
 * Secrets require .toString() to access the actual value.
 */
const email = getVariable('TEST_USER_EMAIL');
const password = getSecret('TEST_PASSWORD').toString();

await page.fill('#email', email);
await page.fill('#password', password);
await page.click('button[type="submit"]');
````

### How do I run tests from CI/CD?

Generate an API key in your job's CI/CD tab, then trigger via HTTP:

```bash
curl -X POST https://your-instance.com/api/jobs/{jobId}/trigger \
  -H "Authorization: Bearer YOUR_API_KEY"
```

### How do I test from different regions?

<Callout type="info">
**Self-Hosted Deployments**: All tests and monitors execute from your local infrastructure. The multi-region selector is available for configuration consistency, but execution occurs sequentially from your single worker location.

**Cloud Deployments**:

- **Performance Tests**:
  - **Playground**: Select specific location (US East, EU Central, or Asia Pacific) for manual test runs
  - **Jobs**: Use global queueâ€”tests execute from any available worker regardless of location
- **Monitors**: Run simultaneously from US East, EU Central, and Asia Pacific using region-specific queues for true geographic distribution
</Callout>
