# SigNoz Observability Deployment Guide

## Architecture Overview

SuperCheck uses SigNoz for observability with two deployment modes:

### Local Development (Single-Node ClickHouse)
- **ClickHouse**: Single-node setup with cluster configuration (single shard, single replica)
- **Storage**: Docker volumes for persistence
- **Schema**: Automatically created via SigNoz schema-migrator
- **Migration**: Uses official SigNoz schema-migrator with all distributed tables

### Production (ClickHouse Cloud)
- **ClickHouse**: Managed ClickHouse Cloud service
- **Storage**: Cloud-managed, replicated across availability zones
- **Schema**: Managed by ClickHouse Cloud
- **Migration**: Use SigNoz schema migrator with cluster support

---

## Local Development Setup

### Prerequisites
- Docker & Docker Compose
- 8GB+ RAM available for Docker
- Ports available: 4317, 4318, 8080, 8123, 9000

### Quick Start

```bash
# Start observability stack
docker compose -f docker-compose.observability.yaml up -d

# Verify services are running
docker compose -f docker-compose.observability.yaml ps

# Expected output:
# - supercheck-clickhouse: healthy
# - supercheck-schema-migrator: exited (completed successfully)
# - supercheck-query-service: healthy
# - supercheck-otel-collector: healthy
```

### Service Endpoints

| Service | Endpoint | Purpose |
|---------|----------|---------|
| SigNoz Query Service | http://localhost:8080 | Query API, used by Next.js app |
| OTel Collector (gRPC) | localhost:4317 | Receive telemetry from services |
| OTel Collector (HTTP) | localhost:4318 | Receive telemetry via HTTP |
| ClickHouse HTTP | http://localhost:8123 | ClickHouse HTTP interface |
| ClickHouse Native | localhost:9000 | ClickHouse native protocol |

### Environment Variables

Add to your `.env` file:

```bash
# SigNoz Query Service
SIGNOZ_URL=http://localhost:8080
SIGNOZ_DISABLE_AUTH=true  # For local dev only!

# OTel Collector endpoints (for instrumenting services)
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318
OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf
```

### Data Persistence

Data is stored in Docker volumes:
- `supercheck_clickhouse_data`: ClickHouse database files
- `supercheck_signoz_data`: SigNoz SQLite metadata (users, dashboards, alerts)

**To reset data:**
```bash
docker compose -f docker-compose.observability.yaml down
docker volume rm supercheck_clickhouse_data supercheck_signoz_data
docker compose -f docker-compose.observability.yaml up -d
```

### Data Retention

Default retention: **72 hours** (configured in table TTL)

To modify, update ClickHouse schema or create retention policies.

---

## Production Deployment with ClickHouse Cloud

### Step 1: Create ClickHouse Cloud Instance

1. Sign up at [ClickHouse Cloud](https://clickhouse.cloud/)
2. Create a new service (Recommended: Production tier with 8GB+ RAM)
3. Note down connection details:
   - Host: `xxxxx.clickhouse.cloud`
   - Port: `8443` (secure) or `9440` (native secure)
   - Username: Usually `default`
   - Password: Generated by ClickHouse Cloud

### Step 2: Update docker-compose for Production

```yaml
# docker-compose.observability.prod.yaml
services:
  # Remove clickhouse service entirely - using ClickHouse Cloud

  # Enable schema migrator for clustered setup
  schema-migrator:
    image: signoz/signoz-schema-migrator:v0.129.8
    container_name: supercheck-schema-migrator
    command:
      - sync
      - --dsn=tcp://your-instance.clickhouse.cloud:9440?secure=true&user=default&password=YOUR_PASSWORD
      - --replication  # Enable for ClickHouse Cloud
    restart: on-failure
    networks:
      - observability

  # SigNoz Query Service
  signoz:
    image: signoz/signoz:v0.100.1
    container_name: supercheck-query-service
    environment:
      - SIGNOZ_TELEMETRYSTORE_CLICKHOUSE_DSN=tcp://your-instance.clickhouse.cloud:9440?secure=true&user=default&password=YOUR_PASSWORD
      - SIGNOZ_SQLSTORE_SQLITE_PATH=/var/lib/signoz/signoz.db
      - STORAGE=clickhouse
      - TELEMETRY_ENABLED=false
      - DEPLOYMENT_TYPE=docker-standalone-amd
    ports:
      - "8080:8080"
    volumes:
      - signoz_data:/var/lib/signoz
    restart: unless-stopped
    networks:
      - observability

  # OTel Collector
  otel-collector:
    image: signoz/signoz-otel-collector:v0.129.8
    container_name: supercheck-otel-collector
    command:
      - --config=/etc/otel/config.yaml
    volumes:
      - ./observability/otel-collector-config.prod.yaml:/etc/otel/config.yaml:ro
    environment:
      - CLICKHOUSE_HOST=your-instance.clickhouse.cloud
      - CLICKHOUSE_PORT=9440
      - CLICKHOUSE_USER=default
      - CLICKHOUSE_PASSWORD=YOUR_PASSWORD
    ports:
      - "4317:4317"
      - "4318:4318"
    depends_on:
      - signoz
    restart: unless-stopped
    networks:
      - observability

volumes:
  signoz_data:
    driver: local

networks:
  observability:
    driver: bridge
```

### Step 3: Update OTel Collector Config for Production

Create `observability/otel-collector-config.prod.yaml`:

```yaml
exporters:
  clickhousetraces:
    datasource: tcp://${CLICKHOUSE_HOST}:${CLICKHOUSE_PORT}/signoz_traces?secure=true&user=${CLICKHOUSE_USER}&password=${CLICKHOUSE_PASSWORD}
    use_new_schema: true

  signozclickhousemetrics:
    dsn: tcp://${CLICKHOUSE_HOST}:${CLICKHOUSE_PORT}/signoz_metrics?secure=true&user=${CLICKHOUSE_USER}&password=${CLICKHOUSE_PASSWORD}

  clickhouselogsexporter:
    dsn: tcp://${CLICKHOUSE_HOST}:${CLICKHOUSE_PORT}/signoz_logs?secure=true&user=${CLICKHOUSE_USER}&password=${CLICKHOUSE_PASSWORD}
    timeout: 10s
    use_new_schema: true

  signozclickhousemeter:
    dsn: tcp://${CLICKHOUSE_HOST}:${CLICKHOUSE_PORT}/signoz_meter?secure=true&user=${CLICKHOUSE_USER}&password=${CLICKHOUSE_PASSWORD}
    timeout: 45s
```

### Step 4: Production Environment Variables

```bash
# SigNoz Query Service (internal network)
SIGNOZ_URL=http://signoz:8080  # Use internal Docker network
SIGNOZ_DISABLE_AUTH=false
SIGNOZ_API_KEY=<generate-strong-random-key>

# OTel Collector (internal network)
OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4318

# ClickHouse Cloud credentials (encrypted in production!)
CLICKHOUSE_HOST=xxxxx.clickhouse.cloud
CLICKHOUSE_PORT=9440
CLICKHOUSE_USER=default
CLICKHOUSE_PASSWORD=<secure-password>
```

### Step 5: Security Hardening

**1. Remove Public Port Exposure**
```yaml
# Only expose SigNoz UI if needed, not ClickHouse or OTel directly
services:
  signoz:
    ports:
      - "127.0.0.1:8080:8080"  # Only localhost access

  otel-collector:
    # Remove ports section - only internal Docker network access
```

**2. Enable Authentication**
```bash
SIGNOZ_DISABLE_AUTH=false
SIGNOZ_API_KEY=$(openssl rand -hex 32)
```

**3. Use TLS for OTel Collector**

Update `otel-collector-config.prod.yaml`:
```yaml
receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
        tls:
          cert_file: /certs/server.crt
          key_file: /certs/server.key
      http:
        endpoint: 0.0.0.0:4318
        tls:
          cert_file: /certs/server.crt
          key_file: /certs/server.key
```

**4. Network Isolation**
```yaml
networks:
  observability:
    driver: bridge
    internal: true  # No external access

  public:
    driver: bridge
```

---

## Migration from Local to ClickHouse Cloud

### Option 1: Export and Import Data

```bash
# Export from local ClickHouse
docker exec supercheck-clickhouse clickhouse-client --query \
  "SELECT * FROM signoz_traces.signoz_traces FORMAT Native" > traces.native

# Import to ClickHouse Cloud
clickhouse-client --host your-instance.clickhouse.cloud \
  --port 9440 --secure --user default --password YOUR_PASSWORD \
  --query "INSERT INTO signoz_traces.signoz_traces FORMAT Native" < traces.native
```

### Option 2: Fresh Start (Recommended)

1. Deploy to ClickHouse Cloud with new instance
2. Start sending telemetry to new instance
3. Historical data remains in local ClickHouse (can be archived)
4. After verification period, decommission local instance

---

## Monitoring and Troubleshooting

### Check Service Health

```bash
# All services
docker compose -f docker-compose.observability.yaml ps

# ClickHouse
docker exec supercheck-clickhouse clickhouse-client --query "SELECT 1"

# SigNoz Query Service
curl http://localhost:8080/api/v1/health

# OTel Collector
curl http://localhost:13133/
```

### View Logs

```bash
# OTel Collector
docker logs supercheck-otel-collector -f

# SigNoz Query Service
docker logs supercheck-query-service -f

# ClickHouse
docker logs supercheck-clickhouse -f
```

### Common Issues

**1. Schema migrator fails**
- **Cause**: ClickHouse cluster configuration not loaded
- **Solution**: Ensure `clickhouse-cluster.xml` is mounted correctly and restart services

**2. No traces appearing in UI**
- **Check**: Is OTel Collector receiving data?
  ```bash
  curl http://localhost:8888/metrics | grep otelcol_receiver_accepted
  ```
- **Check**: Are services sending to correct endpoint?
  - Local: `http://localhost:4318/v1/traces`
  - Docker: `http://otel-collector:4318/v1/traces`

**3. ClickHouse out of memory**
- **Solution**: Increase Docker memory limit or add memory constraints
  ```yaml
  clickhouse:
    deploy:
      resources:
        limits:
          memory: 8G
  ```

**4. Data retention not working**
- **Solution**: TTL is set by schema-migrator automatically to 72 hours
- **Check**: `docker exec supercheck-clickhouse clickhouse-client --query "SHOW CREATE TABLE signoz_traces.signoz_index_v3"`

---

## Cost Optimization

### ClickHouse Cloud Pricing Factors

1. **Compute**: Based on replica size and uptime
2. **Storage**: Per GB stored
3. **Data Transfer**: Ingress usually free, egress charged

### Optimization Strategies

**1. Reduce Data Retention**
```sql
-- Reduce from 72 hours to 24 hours
ALTER TABLE signoz_traces.signoz_traces
  MODIFY TTL toDateTime(timestamp) + INTERVAL 24 HOUR;
```

**2. Sampling**

Update `otel-collector-config.yaml`:
```yaml
processors:
  probabilistic_sampler:
    sampling_percentage: 10  # Keep only 10% of traces

service:
  pipelines:
    traces:
      processors: [probabilistic_sampler, batch]
```

**3. Compression**

Tables already use optimal compression (ZSTD, DoubleDelta), but you can adjust:
```sql
-- Even higher compression at cost of CPU
ALTER TABLE signoz_traces.signoz_traces
  MODIFY COLUMN trace_id String CODEC(ZSTD(9));
```

---

## Performance Tuning

### ClickHouse Settings

```yaml
clickhouse:
  environment:
    - MAX_MEMORY_USAGE=8000000000  # 8GB
    - MAX_THREADS=4
    - MAX_CONCURRENT_QUERIES=50
```

### OTel Collector Tuning

```yaml
processors:
  batch:
    send_batch_size: 10000
    send_batch_max_size: 11000
    timeout: 10s
```

---

## Backup and Disaster Recovery

### Local Development

```bash
# Backup Docker volumes
docker run --rm -v supercheck_clickhouse_data:/data -v $(pwd):/backup \
  ubuntu tar czf /backup/clickhouse-backup.tar.gz /data

# Restore
docker run --rm -v supercheck_clickhouse_data:/data -v $(pwd):/backup \
  ubuntu tar xzf /backup/clickhouse-backup.tar.gz -C /
```

### ClickHouse Cloud

- **Automatic Backups**: ClickHouse Cloud provides automatic backups
- **Point-in-Time Recovery**: Available on higher tiers
- **Replication**: Cross-region replication available

---

## Next Steps

1. ✅ **Local Setup Complete** - Services are running
2. ⏳ **Instrument Services** - Add OpenTelemetry to worker and app
3. ⏳ **Test Telemetry Flow** - Send test traces/logs/metrics
4. ⏳ **Configure Alerts** - Set up alerting rules
5. ⏳ **Production Deployment** - Migrate to ClickHouse Cloud

See main [README.md](./README.md) for instrumentation examples.
